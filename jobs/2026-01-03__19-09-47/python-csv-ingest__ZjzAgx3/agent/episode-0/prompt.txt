You are an AI assistant tasked with solving command-line tasks in a Linux environment. You will be given a task description and the output from previously executed commands. Your goal is to solve the task by providing batches of shell commands.

Format your response as JSON with the following structure:

{
  "analysis": "Analyze the current state based on the terminal output provided. What do you see? What has been accomplished? What still needs to be done?",
  "plan": "Describe your plan for the next steps. What commands will you run and why? Be specific about what you expect each command to accomplish.",
  "commands": [
    {
      "keystrokes": "ls -la\n",
      "duration": 0.1
    },
    {
      "keystrokes": "cd project\n",
      "duration": 0.1
    }
  ],
  "task_complete": true
}

Required fields:
- "analysis": Your analysis of the current situation
- "plan": Your plan for the next steps
- "commands": Array of command objects to execute

Optional fields:
- "task_complete": Boolean indicating if the task is complete (defaults to false if not present)

Command object structure:
- "keystrokes": String containing the exact keystrokes to send to the terminal (required)
- "duration": Number of seconds to wait for the command to complete before the next command will be executed (defaults to 1.0 if not present)

IMPORTANT: The text inside "keystrokes" will be used completely verbatim as keystrokes. Write commands exactly as you want them sent to the terminal:
- Most bash commands should end with a newline (\n) to cause them to execute
- For special key sequences, use tmux-style escape sequences:
  - C-c for Ctrl+C
  - C-d for Ctrl+D

The "duration" attribute specifies the number of seconds to wait for the command to complete (default: 1.0) before the next command will be executed. On immediate tasks (e.g., cd, ls, echo, cat) set a duration of 0.1 seconds. On commands (e.g., gcc, find, rustc) set a duration of 1.0 seconds. On slow commands (e.g., make, python3 [long running script], wget [file]) set an appropriate duration as you determine necessary.

It is better to set a smaller duration than a longer duration. It is always possible to wait again if the prior output has not finished, by running {"keystrokes": "", "duration": 10.0} on subsequent requests to wait longer. Never wait longer than 60 seconds; prefer to poll to see intermediate result status.

Important notes:
- Each command's keystrokes are sent exactly as written to the terminal
- Do not include extra whitespace before or after the keystrokes unless it's part of the intended command
- Extra text before or after the JSON will generate warnings but be tolerated
- The JSON must be valid - use proper escaping for quotes and special characters within strings
- Commands array can be empty if you want to wait without taking action

Task Description:
## Overview

Implement a Python CLI that ingests multiple CSV inputs with inconsistent schemas (varying names, encodings, date formats), standardizes column names, ISO-formats dates, imputes missing numericals by median and categoricals as 'Unknown', clips numeric outliers at the 1st/99th percentiles, and outputs a consolidated cleaned dataset and a JSON log of applied cleaning operations.

### Requirements
- Python CLI that ingests multiple CSV inputs
- Inputs must have inconsistent schemas (varying names, encodings, date formats)
- Must have standardizes column names, ISO-formats dates
- Inputs missing numericals by median and categoricals as 'Unknown'
- Clip the numeric outliers at the 1st/99th percentiles
- Should output a consolidated cleaned dataset
- Have a JSON log of applied cleaning operations

## System Requirements

### Required Software
- **Python**: 3.8 or higher
- **Bash**: 4.0 or higher
- **pip**: Python package manager

### Python Dependencies
```bash

pytest==8.4.1 \# For testing
argparse==1.4.0 \
datetime==5.5 \
pandas>=2.0.0
numpy>=1.24.0
pathlib==1.0.1 \
 typing==3.10.0.0 
```

---

## Installation

### 2. Install Python Dependencies
```bash
# Create virtual environment (recommended)
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install pandas numpy pytest
```

### 3. Make Scripts Executable
```bash
chmod +x solution/CSVIngester.py
chmod +x solution/solve.sh
chmod +x tests/test.sh
```

---

## Project Structure

```
python-csv-ingest/
        
├── solution
|   |__ CSVIngester.py          # Main Python CLI application
|   |__ solve.sh                # Bash shell interface
├── tests
|    |__ test.sh                # Bash shell to run test interface
|   |__test_outputs.py            # Pytest test suite
|   ├──test_data.csv              # Generated test file
|   ├──test2_data.csv           # Generated test file
|   ├──test3_data.csv          # Generated test file
|   ├──cleaned_data.csv            # Output file (generated and removed after test case completion)
|   └──cleaning_log.json           # Operation log (generated and removed after test case completion)
|   └──final_log.json           # Comprehensive Operation log (generated and removed after test case completion)
├── INSTRUCTIONS.md             # This file

```

---

## Core Components

### 1. CSV Ingester (`CSVIngester.py`)

**Main Class: `CSVIngester`**

**Key Methods:**
- `encode_process()` - Auto-detects file encoding (UTF-8, Latin-1, etc.)
- `standardize_column_name()` - Converts columns to snake_case
- `find_column_type()` - Identifies numeric/date/categorical columns
- `date_parser()` - Converts various date formats to ISO-8601
- `outlier_truncate()` - Clips values at 1st/99th percentiles
- `logging_process()` - Output a json log of the cleaned process
- `get_operations_log()` - Helper fucntion to output json logs
- `processed_dataframe()` - Cleans and process a single CSV file
- `consolidated_cleaned_dataframes()` - Merges multiple cleaned CSV file 
- `file_processor()` - Full pipeline execution

**Features:**
- ✅ Handles multiple encodings (UTF-8, Latin-1, ISO-8859-1, CP1252)
- ✅ Standardizes inconsistent column names
- ✅ Detects and parses 14+ date formats
- ✅ Fills missing numerics with median
- ✅ Fills missing categoricals with "Unknown"
- ✅ Clips outliers at 1st/99th percentiles
- ✅ Generates detailed JSON operation logs

### 2. Shell Interface (`solution/solve.sh`)

**Available Bash Functions:**
- `encode_process <filepath>`
- `standardize_column_name <column_name>`
- `find_column_type <csv_file>findmn_name>`
- `date_parser <csv_file> <column_name>`
- `outlier_truncate <csv_file> <column_name>`
- `processed_dataframe <csv_file> [output_file]`
- `consolidated_cleaned_dataframes <output_file> <file1> <file2> ...`
- `file_processor <output_file> <log_file> <file1> <file2> ...`
- `logging_process [log_file]`
- `get_csv_summary <csv_file>`
- `operations_logs <output_file>`

### 3. Test Data Generator (`generate_test_csvs.py`)

Three already generated messy CSV files for testing:
- **test_data.csv** (10 rows)
- **test2_data.csv** (10 rows)
- **test3_data.csv** (10 rows)

---

## Usage Guide

### Quick Start

#### 1. Clean Data Using Python CLI
```bash
# Basic usage
python solution/CSVIngester.py tests/test_data.csv tests/test2_data.csv tests/test3_data.csv

# Custom output paths
python solution/CSVIngester.py tests/test_data.csv tests/test2_data.csv -o tests/cleaned.csv -l tests/log.json

# View help
python solution/CSVIngester.py --help
```

#### 3. Clean Data Using Bash Functions
```bash
# Source the shell script
source solution/solve.sh

# Use individual functions
encode_process "tests/test_data.csv"
standardize_column_name "Product Price $"
find_column_type "tests/test_data.csv" "Order Date"

# Full pipeline
file_processor "output.csv" "log.json" "tests/test_data.csv" "tests/test2_data.csv"
```

### Advanced Usage

#### Inspect CSV Before Cleaning
```bash
source solution/solve.sh
get_csv_summary "tests/test_data.csv"
```

Output (JSON):
```json
{
  "file": "tests/test_data.csv",
  "rows": 10,
  "columns": 8,
  "column_names": ["Order ID", "Customer Name", "Order Date", ...],
  "missing_values": {"Customer Name": 2, "Quantity!!": 10, ...}
}
```

#### Check Column Typfindbash
find_column_type "tests/test_data.csv" "Order Date"  # Returns: date
find_column_type "tests/test_data.csv" "Product Price $"  # Returns: numeric
find_column_type "tests/test_data.csv" "Status"  # Returns: categorical
```

#### Analyze Outliers
```bash
outlier_truncate "tests/test_data.csv" "Product Price $"
```

Output (JSON):
```json
{
  "lower_bound": 15.5,
  "upper_bound": 485.2,
  "original_min": 10.0,
  "original_max": 9500.0,
  "clipped_min": 15.5,
  "clipped_max": 485.2
}
```

#### Clean Single File
```bash
processed_dataframe "tests/test_data.csv" "tests/cleaned_output.csv"
```

#### Consolidate Multiple Files
```bash
consolidated_cleaned_dataframes "consolidated_output.csv" "tests/test_data.csv" "tests/test2_data.csv" "tests/test3_data.csv"
```

#### View Cleaning Log
```bash
process_files "output.csv" "log.json" "tests/test_data.csv"
get_cleaning_log "log.json"
```

Output (JSON):
```json
{
  "timestamp": "2025-01-03T10:30:45.123456",
  "operations": [
   {
      "operation": "load_file",
      "details": {
        "source": "tests/test_data.csv",
        "rows": 10,
        "columns": 8
      },
      "timestamp": "2026-01-03T11:15:21.457038"
    },
    {
      "operation": "standardize_columns",
      "details": {
        "source": "tests/test_data.csv",
        "mappings": {
          "Order ID": "order_id",
          "Customer Name": "customer_name",
          "Order Date": "order_date",
          "Product Price $": "product_price",
          "Quantity!!": "quantity",
          "Total Amount": "total_amount",
          "Ship Date": "ship_date",
          "Status": "status"
        }
      },
      "timestamp": "2026-01-03T11:15:21.457205"
    }
    ...
  ]
}
```

---

## Testing

### Running Tests

#### Run All Tests
```bash
pytest tests/test_outputs.py -v
```

#### Run Specific Test
```bash
pytest tests/test_outputs.py::test_should_detect_encoding -v
pytest tests/test_outputs.py::test_get_cleaning_log -v
```

#### Run with Detailed Output
```bash
pytest tests/test_outputs.py -vv --tb=short
```

#### Run with Coverage (Optional)
```bash
pip install pytest-cov
pytest tests/test_outputs.py --cov=csv_cleaner --cov-report=html
```

### Test Suite Overview

**Total Tests:** 23
## Test Cases

### Test Case 1: Column Name Standardization

**Input:**
```python
"Product Price $"
"Order ID"
"Quantity!!"
"Customer Name"
```

**Expected Output:**
```python
"product_price"
"order_id"
"quantity"
"customer_name"
```

**Test:**
```bash
pytest tests/test_outputs.py::test_standardize_spaces_col_name -v  
pytest tests/test_outputs.py::test_standardize_any_special_chars -v
pytest tests/test_outputs.py::test_standardize_any_casing -v
```

---

### Test Case 2: Date Format Detection

**Input CSV:**
```csv
Order Date
2025-01-01
01/05/2025
Jan 10, 2025
15-01-2025
2025/01/20
```

**Expected:**
- Column type detected as: `date`
- All dates converted to: `YYYY-MM-DD` format

**Test:**
```bash
pytest tests/test_outputs.py::test_detect_date_column -v
pytest tests/test_outputs.py::test_parse_iso_dates -v
pytest tests/test_outputs.py::test_parse_mixed_date_formats -v
```

### Test Case 3: Missing Value Imputation

**Input CSV:**
```csv
ID,Price,Quantity,Category
1,100,5,Electronics
2,150,,Furniture
3,,10,Electronics
4,120,8,
```

**Expected Output:**
- Price (numeric): Missing filled with median (120)
- Quantity (numeric): Missing filled with median (7.5)
- Category (categorical): Missing filled with "Unknown"

**Test:**
```bash
pytest tests/test_outputs.py::test_clean_single_dataframe -v
pytest tests/test_outputs.py::test_cleaned_columns_standardized -v
```
---

### Test Case 4: Outlier Clipping

**Input CSV:**
```csv
Product,Price
Widget,100
Gadget,150
Outlier,9999
Normal,120
```

**Expected:**
- Outliers (9999) clipped to 99th percentile
- 1% lowest values clipped to 1st percentile
- Log shows clipping operation

**Test:**
```bash
pytest tests/test_outputs.py::test_clip_numeric_outliers -v
```

Expected JSON:
```json
{
  "lower_bound": 15.5,
  "upper_bound": 485.2,
  "original_min": 10.0,
  "original_max": 9500.0,
  "clipped_min": 15.5,
  "clipped_max": 485.2
}
```

---

### Test Case 5: Multi-File Consolidation

**Input:**
- `tests/test_data.csv` (150 rows, 8 columns)
- `employee_data.csv` (100 rows, 7 columns)
- `inventory_data.csv` (80 rows, 7 columns)

**Expected:**
- All files merged into single CSV
- Total rows: 330
- Columns: Union of all unique columns
- Missing columns filled with NaN

**Test:**
```bash
pytest tests/test_outputs.py::test_consolidate_dataframes -v
```

---

### Test Case 6: Encoding Detection

**Input:**
- `tests/test_data.csv` (UTF-8 encoding)

**Expected:**
- UTF-8 detected for tests/test_data.csv
- Latin-1 detected for employee_data.csv
- Both files read correctly

**Test:**
```bash
pytest tests/test_outputs.py::test_should_detect_encoding -v
pytest tests/test_outputs.py::test_should_detect_encoding_nonexistent_file -v
```

---

### Test Case 7: Full Pipeline Execution

**Input:**
- Multiple CSV files with various issues
- Inconsistent schemas
- Missing values
- Outliers
- Multiple date formats

**Expected Output:**
1. Cleaned and consolidated CSV
2. Detailed JSON log with all operations
3. Standardized column names
4. All dates in ISO format
5. Missing values filled
6. Outliers clipped

**Test:**
```bash
pytest tests/test_outputs.py::test_process_full_pipeline -v
pytest tests/test_outputs.py::test_full_workflow -v
```
---

### Test Case 8: Column Type Detection Accuracy

**Input CSV:**
```csv
ID,Date,Amount,Status
1,2025-01-01,100.50,Active
2,01/05/2025,200.75,Pending
3,Jan 10 2025,150.25,Active
```

**Expected:**
- ID: `numeric`
- Date: `date`
- Amount: `numeric`
- Status: `categorical`

**Test:**
```bash
pytest tests/test_outputs.py::test_detect_numeric_column -v
pytest tests/test_outputs.py::test_detect_categorical_column -v

```

---

### Test Case 9: Error Handling

**Scenarios:**
1. Non-existent file
2. Non-existent column
3. Invalid CSV format
4. Empty CSV

**Expected:**
- Graceful error messages
- No crashes
- Appropriate return codes

**Test:**
```bash
pytest tests/test_outputs.py::test_detect_nonexistent_column -v
pytest tests/test_outputs.py::test_get_cleaning_log_nonexistent_file -v
pytest tests/test_outputs.py::test_should_detect_encoding_nonexistent_file -v
pytest tests/test_outputs.py::test_summary_shows_missing_values -v
```

---

### Test Case 10: CSV Summary

**Input:**
- CSV files

**Expected Output:**
1.A summary of the csv file


**Test:**
```bash
pytest tests/test_solution.py::test_get_csv_summary -v
pytest tests/test_solution.py::test_summary_shows_missing_values -v
```
--

### Test Case 10: CSV Summary

**Input:**
- CSV files

**Expected Output:**
1.Test retrieval of existing operations from log
2.Check logs contains an operation


**Test:**
```bash
pytest tests/test_solution.py::test_get_existing_operations -v
pytest tests/test_solution.py::test_process_log_contains_operations -v
```

## Additional Resources

### Supported Date Formats

The system automatically detects and parses these formats:
- ISO: `2025-01-01`, `2025/01/01`, `2025.01.01`
- US: `01/15/2025`, `01/15/25`, `1-15-2025`
- European: `15/01/2025`, `15.01.2025`, `15-01-2025`
- Text: `Jan 15, 2025`, `15 Jan 2025`, `January 15, 2025`

### Column Name Transformations

| Original | Standardized |
|----------|-------------|
| `Product Name` | `product_name` |
| `Price $` | `price` |
| `Quantity!!` | `quantity` |
| `Order-ID` | `order_id` |
| `Customer_Email` | `customer_email` |

### Performance Tips

1. **Large Files:** Process in batches
2. **Many Columns:** Consider selective cleaning
3. **Memory Issues:** Increase system swap space
4. **Speed:** Use SSD for temporary files

Current terminal state:
Current Terminal Screen:
bash: cannot set terminal process group (-1): Inappropriate ioctl for device
bash: no job control in this shell
root@7186cdd8cb2e:/app#








































