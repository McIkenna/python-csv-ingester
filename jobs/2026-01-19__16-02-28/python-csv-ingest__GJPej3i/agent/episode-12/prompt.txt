New Terminal Output:

root@5c43413d2e06:/app/tests# cd /app
root@5c43413d2e06:/app# cat > solution/CSVIngester.py << 'EOFPYTHON'
d-%m-%y',      # 01-0> #!/usr/bin/env python3
1-23
            '%d/%m>
/> import argparse
> import pandas as pd
> import numpy as np
> import json
    > import re
> from datetime import datetime
> from pathlib import Path
> from typing import List, Dict, Any, Optional, Tuple
, 2023
            '%d %b %Y',      # 01 Jan 2023
            '%B %d, %Y',     # Janua>
r>
> class CSVIngester:
>     """Main class for CSV data ingestion, cleaning, and consolidation."""
>
>     def __init__(self):
>         self.operations_log = []
>
>     def encode_process(self, filepath: str) -> Optional[str]:
alueError:
           >         """Auto-detect file encoding (UTF-8, Latin-1)."""
>         encodings = ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252']
>
>         if not Path(filepath).exists():
>             return None
>
notna(dt):
                return dt.strftime('%Y-%>         for encoding in encodings:
>             try:
>                 with open(filepath, 'r', encoding=encoding) as f:
>                     f.read()
>                 return encoding
>             except (UnicodeDecodeError, UnicodeError):
>                 continue
>
>         return 'utf-8'  # default fallback
>
>     def standardize_column_name(self, column_name: str) -> str:
>         """Convert column names to snake_case."""
>         # First replace underscores with spaces to normalize
>         name = column_name.replace('_', ' ')
>         # Remove special characters (keep only alphanumeric and spaces)
>         name = re.sub(r'[^a-zA-Z0-9\s]', '', name)
>         # Replace multiple spaces with single space
>         name = re.sub(r'\s+', ' ', name)
>         # Strip and convert to lowercase
>         name = name.strip().lower()
>         # Replace spaces with underscores
'original_min': float(original_min),
            'original_max': float(original_max),
            'clipped_min': float(clipped.mi>         name = name.replace(' ', '_')
>         return name
>
>     def detect_column_type(self, df: pd.DataFrame, column_name: str) -> Optional[str]:
>         """Identify column type: numeric, date, or categorical."""
>         if column_name not in df.columns:
>             return None
ils': details,
         >
>         col = df[column_name]
>
>         # Try to detect date column
>         if col.dtype == 'object':
>             # Sample non-null values
>             sample = col.dropna().head(20)
>             if len(sample) > 0:
>                 date_count = 0
>                 for val in sample:
>                     if self._is_date(str(val)):
>                         date_count += 1
>
>                 # If more than 50% look like dates, it's a date column
>                 if date_count / len(sample) > 0.5:
>                     return 'date'
>
>         # Try to detect numeric column
>         try:
>             pd.to_numeric(col, errors='coerce')
>             # Check if mostly numeric
>             numeric_count = pd.to_numeric(col, errors='coerce').notna().sum()
d>             if numeric_count / len(col) > 0.5:
>                 return 'numeric'
>         except:
>             pass
>
>         # Default to categorical
>         return 'categorical'
>
>     def _is_date(self, value: str) -> bool:
>         """Helper to check if a string looks like a date."""
>         date_patterns = [
 self.logging_proces>             r'\d{4}-\d{1,2}-\d{1,2}',  # 2023-01-01
s('standardize_columns', {
            'source': fil>             r'\d{4}/\d{1,2}/\d{1,2}',  # 2023/01/01
>             r'\d{4}\.\d{1,2}\.\d{1,2}',  # 2023.01.01
>             r'\d{1,2}-\d{1,2}-\d{4}',  # 01-01-2023
>             r'\d{1,2}/\d{1,2}/\d{4}',  # 01/01/2023
>             r'\d{1,2}\.\d{1,2}\.\d{4}',  # 01.01.2023
>         ]
>
>         for pattern in date_patterns:
>             if re.match(pattern, value.strip()):
>                 return True
>
>         return False
>
>     def date_parser(self, date_str: str) -> Optional[str]:
>         """Convert various date formats to ISO-8601 (YYYY-MM-DD)."""
>         if pd.isna(date_str) or date_str == '':
>             return None
>
>         date_str = str(date_str).strip()
>
>         # List of date formats to try
>         date_formats = [
>             '%Y-%m-%d',      # 2023-01-01
>             '%Y/%m/%d',      # 2023/01/01
>             '%Y.%m.%d',      # 2023.01.01
>             '%d-%m-%Y',      # 01-01-2023
>             '%d/%m/%Y',      # 01/01/2023
>             '%d.%m.%Y',      # 01.01.2023
>             '%m-%d-%Y',      # 01-01-2023 (US format)
>             '%m/%d/%Y',      # 01/01/2023 (US format)
>             '%d-%m-%y',      # 01-01-23
>             '%d/%m/%y',      # 01/01/23
>             '%m/%d/%y',      # 01/01/23
>             '%b %d, %Y',     # Jan 01, 2023
               # Impute missing with median
                if df[col].isna().any():
                    median_val = d
[... output limited to 10000 bytes; 70 interior bytes omitted ...]
:
>             col_type = self.detect_column_type(df, col)
>
>             if col_type == 'date':
>                 # Parse dates
>                 df[col] = df[col].apply(self.date_parser)
     help='Output>                 self.logging_process('parse_dates', {
>                     'column': col,
csv)'
    )

    parser.add_argument(
 >                     'format': 'ISO-8601'
>                 })
>
      help='>             elif col_type == 'numeric':
>                 # Convert to numeric
>                 df[col] = pd.to_numeric(df[col], errors='coerce')
>
>                 # Impute missing with median
>                 if df[col].isna().any():
>                     median_val = df[col].median()

    print(f"Cleaned data saved t>                     missing_count = df[col].isna().sum()
>                     df[col] = df[col].fillna(median_val)
>
>                     self.logging_process('impute_numeric', {
>                         'column': col,
>                         'method': 'median',
>                         'value': float(median_val) if pd.notna(median_val) else 0.0,
>                         'missing_count': int(missing_count)
>                     })
>
>                 # Clip outliers
>                 lower = df[col].quantile(0.01)
>                 upper = df[col].quantile(0.99)
>                 df[col] = df[col].clip(lower=lower, upper=upper)
>
>                 self.logging_process('clip_outliers', {
>                     'column': col,
>                     'lower_percentile': 1,
>                     'upper_percentile': 99,
>                     'lower_bound': float(lower),
>                     'upper_bound': float(upper)
>                 })
>
>             elif col_type == 'categorical':
>                 # Impute missing with 'Unknown'
>                 if df[col].isna().any() or (df[col] == '').any():
>                     missing_count = df[col].isna().sum() + (df[col] == '').sum()
>                     df[col] = df[col].fillna('Unknown')
>                     df[col] = df[col].replace('', 'Unknown')
>
>                     self.logging_process('impute_categorical', {
>                         'column': col,
>                         'method': 'constant',
>                         'value': 'Unknown',
>                         'missing_count': int(missing_count)
>                     })
>
>         return df
>
>     def consolidated_cleaned_dataframes(self, dataframes: List[pd.DataFrame]) -> pd.DataFrame:
>         """Merge multiple cleaned DataFrames."""
>         if not dataframes:
>             return pd.DataFrame()
>
>         # Concatenate all dataframes
>         consolidated = pd.concat(dataframes, ignore_index=True)
>
>         self.logging_process('consolidate_dataframes', {
>             'total_files': len(dataframes),
>             'total_rows': len(consolidated),
>             'total_columns': len(consolidated.columns)
>         })
>
>         return consolidated
>
>     def file_processor(self, input_files: List[str], output_file: str, log_file: str):
>         """Full pipeline execution."""
>         self.operations_log = []  # Reset log
>
>         # Process each file
>         dataframes = []
>         for filepath in input_files:
>             df = self.processed_dataframe(filepath)
>             dataframes.append(df)
>
>         # Consolidate
>         consolidated = self.consolidated_cleaned_dataframes(dataframes)
>
>         # Save output
>         consolidated.to_csv(output_file, index=False)
>
>         self.logging_process('save_output', {
>             'output_file': output_file,
>             'rows': len(consolidated),
>             'columns': len(consolidated.columns)
>         })
>
>         # Save log
>         log_data = {
>             'timestamp': datetime.now().isoformat(),
>             'operations': self.operations_log
>         }
>
>         with open(log_file, 'w') as f:
>             json.dump(log_data, f, indent=2)
>
>         return consolidated
>
>
> def main():
>     parser = argparse.ArgumentParser(
>         description='CSV Ingester - Clean and consolidate multiple CSV files'
>     )
>
>     parser.add_argument(
>         'input_files',
>         nargs='+',
>         help='Input CSV files to process'
>     )
>
>     parser.add_argument(
>         '-o', '--output',
>         default='cleaned_data.csv',
>         help='Output file for cleaned data (default: cleaned_data.csv)'
>     )
>
>     parser.add_argument(
>         '-l', '--log',
>         default='cleaning_log.json',
>         help='Output file for cleaning log (default: cleaning_log.json)'
>     )
>
>     args = parser.parse_args()
>
>     # Process files
>     ingester = CSVIngester()
>     ingester.file_processor(args.input_files, args.output, args.log)
>
>     print(f"Processing complete!")
>     print(f"Cleaned data saved to: {args.output}")
>     print(f"Cleaning log saved to: {args.log}")
>
>
> if __name__ == '__main__':
>     main()
> EOFPYTHON
root@5c43413d2e06:/app#
